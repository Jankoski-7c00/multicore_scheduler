{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)\\nconv2d_nchw_nn, conv2d_nchw_ff, conv2d_nchw_yy, conv2d_nchw_xx, conv2d_nchw_rc, conv2d_nchw_ry, conv2d_nchw_rx = tuple(conv2d_nchw.op.axis) + tuple(conv2d_nchw.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_subtract_ax0, T_subtract_ax1, T_subtract_ax2, T_subtract_ax3 = tuple(T_subtract.op.axis) + tuple(T_subtract.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\nT_divide_ax0, T_divide_ax1, T_divide_ax2, T_divide_ax3 = tuple(T_divide.op.axis) + tuple(T_divide.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_multiply_ax0, T_multiply_ax1, T_multiply_ax2, T_multiply_ax3 = tuple(T_multiply.op.axis) + tuple(T_multiply.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\npad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)\\nconv2d_nchw_nn, conv2d_nchw_ff, conv2d_nchw_yy, conv2d_nchw_xx, conv2d_nchw_rc, conv2d_nchw_ry, conv2d_nchw_rx = tuple(conv2d_nchw.op.axis) + tuple(conv2d_nchw.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_subtract_ax0, T_subtract_ax1, T_subtract_ax2, T_subtract_ax3 = tuple(T_subtract.op.axis) + tuple(T_subtract.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\nT_divide_ax0, T_divide_ax1, T_divide_ax2, T_divide_ax3 = tuple(T_divide.op.axis) + tuple(T_divide.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_multiply_ax0, T_multiply_ax1, T_multiply_ax2, T_multiply_ax3 = tuple(T_multiply.op.axis) + tuple(T_multiply.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\npad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)\\nconv2d_nchw_nn, conv2d_nchw_ff, conv2d_nchw_yy, conv2d_nchw_xx, conv2d_nchw_rc, conv2d_nchw_ry, conv2d_nchw_rx = tuple(conv2d_nchw.op.axis) + tuple(conv2d_nchw.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_subtract_ax0, T_subtract_ax1, T_subtract_ax2, T_subtract_ax3 = tuple(T_subtract.op.axis) + tuple(T_subtract.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\nT_divide_ax0, T_divide_ax1, T_divide_ax2, T_divide_ax3 = tuple(T_divide.op.axis) + tuple(T_divide.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_multiply_ax0, T_multiply_ax1, T_multiply_ax2, T_multiply_ax3 = tuple(T_multiply.op.axis) + tuple(T_multiply.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\npad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)\\nconv2d_nchw_nn, conv2d_nchw_ff, conv2d_nchw_yy, conv2d_nchw_xx, conv2d_nchw_rc, conv2d_nchw_ry, conv2d_nchw_rx = tuple(conv2d_nchw.op.axis) + tuple(conv2d_nchw.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_subtract_ax0, T_subtract_ax1, T_subtract_ax2, T_subtract_ax3 = tuple(T_subtract.op.axis) + tuple(T_subtract.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\nT_divide_ax0, T_divide_ax1, T_divide_ax2, T_divide_ax3 = tuple(T_divide.op.axis) + tuple(T_divide.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_multiply_ax0, T_multiply_ax1, T_multiply_ax2, T_multiply_ax3 = tuple(T_multiply.op.axis) + tuple(T_multiply.op.reduce_axis)\\nT_reshape_ax0, T_reshape_ax1, T_reshape_ax2, T_reshape_ax3 = tuple(T_reshape.op.axis) + tuple(T_reshape.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\nT_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)\\ncompute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tvm import topi\n",
    "from tvm import te\n",
    "from tvm.contrib import tedd\n",
    "from tvm.auto_scheduler import ComputeDAG\n",
    "\n",
    "def img2col(input: te.Tensor, kernel_size: tuple, stride: int, padding: tuple) -> te.Tensor:\n",
    "    \"\"\"\n",
    "    input layout: NHWC\n",
    "    \"\"\"\n",
    "    n, h, w, c = input.shape\n",
    "    kernel_h, kernel_w = kernel_size\n",
    "    assert n == 1\n",
    "    out_w = kernel_h * kernel_w * c\n",
    "    out_h = h * w // stride\n",
    "    pad_h, pad_w = padding\n",
    "\n",
    "    #return te.compute((out_h, out_w), lambda ii, jj: te.if_then_else(\n",
    "    #    1,\n",
    "    #    0,\n",
    "    #    input[0, , ,jj % c]\n",
    "    #), name = \"img2col\")\n",
    "\n",
    "input = te.placeholder(shape = (1, 64, 56, 56), name = \"input\")\n",
    "weights_0 = te.placeholder(shape = (64, 1, 1, 1), name = \"weights_0\")\n",
    "weights_1 = te.placeholder(shape = (64, 64, 3, 3), name = \"weights_1\")\n",
    "weights_2 = te.placeholder(shape = (256, 64, 1, 1), name = \"weights_2\")\n",
    "weights_shortcut = te.placeholder(shape = (256, 1, 1, 1), name = \"weights_shortcut\")\n",
    "\n",
    "mean_0 = te.placeholder(shape = (64,), name = \"mean_0\")\n",
    "mean_1 = te.placeholder(shape = (64,), name = \"mean_1\")\n",
    "mean_2 = te.placeholder(shape = (256,), name = \"mean_2\")\n",
    "mean_shortcut = te.placeholder(shape = (256,), name = \"mean_shortcut\")\n",
    "\n",
    "var_0 = te.placeholder(shape = (64,), name = \"var_0\")\n",
    "var_1 = te.placeholder(shape = (64,), name = \"var_1\")\n",
    "var_2 = te.placeholder(shape = (256,), name = \"var_2\")\n",
    "var_shortcut = te.placeholder(shape = (256,), name = \"var_shortcut\")\n",
    "\n",
    "gamma_0 = te.placeholder(shape = (64,), name = \"gamma_0\")\n",
    "gamma_1 = te.placeholder(shape = (64,), name = \"gamma_1\")\n",
    "gamma_2 = te.placeholder(shape = (256,), name = \"gamma_2\")\n",
    "gamma_shortcut = te.placeholder(shape = (256,), name = \"gamma_shortcut\")\n",
    "\n",
    "beta_0 = te.placeholder(shape = (64,), name = \"beta_0\")\n",
    "beta_1 = te.placeholder(shape = (64,), name = \"beta_1\")\n",
    "beta_2 = te.placeholder(shape = (256,), name = \"beta_2\")\n",
    "beta_shortcut = te.placeholder(shape = (256,), name = \"beta_shortcut\")\n",
    "\n",
    "#layer0\n",
    "conv0 = topi.nn.conv2d(input, weights_0, 1, (0, 0), 1)\n",
    "bn0, _, _ = topi.nn.batch_norm(conv0, gamma_0, beta_0, mean_0, var_0)\n",
    "relu0 = topi.nn.relu(bn0)\n",
    "\n",
    "#layer1\n",
    "conv1 = topi.nn.conv2d(relu0, weights_1, 1, (1, 1), 1)\n",
    "bn1, _, _ = topi.nn.batch_norm(conv1, gamma_1, beta_1, mean_1, var_1)\n",
    "relu1 = topi.nn.relu(bn1)\n",
    "\n",
    "#layer2\n",
    "conv2 = topi.nn.conv2d(relu1, weights_2, 1, (0, 0), 1)\n",
    "bn2, _, _ = topi.nn.batch_norm(conv2, gamma_2, beta_2, mean_2, var_2)\n",
    "\n",
    "#shortcut\n",
    "conv_shortcut = topi.nn.conv2d(input, weights_shortcut, 1, (0, 0), 1)\n",
    "bn_shortcut, _, _ = topi.nn.batch_norm(conv_shortcut, gamma_shortcut, beta_shortcut, mean_shortcut, var_shortcut)\n",
    "\n",
    "#output\n",
    "out_add = topi.add(bn2, bn_shortcut)\n",
    "out_relu = topi.nn.relu(out_add)\n",
    "\n",
    "sch = te.create_schedule(out_relu.op)\n",
    "\n",
    "'''\n",
    "print(tvm.lower(sch, [input, weights_0, weights_1, weights_2, weights_shortcut,\n",
    "                      mean_0, mean_1, mean_2, mean_shortcut,\n",
    "                      var_0, var_1, var_2, var_shortcut,\n",
    "                      gamma_0, gamma_1, gamma_2, gamma_shortcut,\n",
    "                      beta_0, beta_1, beta_2, beta_shortcut, out_relu], simple_mode = True))\n",
    "'''\n",
    "#tedd.viz_dataflow_graph(sch,show_svg = True)\n",
    "dag = ComputeDAG([input, weights_0, weights_1, weights_2, weights_shortcut,\n",
    "                      mean_0, mean_1, mean_2, mean_shortcut,\n",
    "                      var_0, var_1, var_2, var_shortcut,\n",
    "                      gamma_0, gamma_1, gamma_2, gamma_shortcut,\n",
    "                      beta_0, beta_1, beta_2, beta_shortcut, out_relu])\n",
    "dag.print_python_code_from_state(dag.get_init_state())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
